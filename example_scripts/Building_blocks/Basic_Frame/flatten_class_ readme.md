The basic principle behind this approach to extracting data from a Zooniverse project built with the project builder is that the project owner knows the structure of their project and need only pick the appropriate functions they need to handle their specific project’s data.  There is no attempt to make a script that will handle every possible project, but rather to provide a basic framework to which the owner can add the necessary tools to pull the data they want from the Zooniverse classification file.

For instance, if they have a question task, they will need to add a block to the basic frame work that will retrieve the answer for that question into a suitably titled field in the output file.  If they have a drawing tool task then they would need to select the correct block for the type of drawing tool they have.  There will also be blocks for survey and transcription tasks, but those these are not yet written as of  08/15.17.

Slicing the classification file – Normally I would recommend the classification file is cleaned and sliced as a first step separate from the flattening operation.  This usually includes selecting specific workflow_id or version, testing for multiple classifications done by the same user, and other possible issues such as stripping off development or beta testing classifications, or classifications done after a certain date.  However, this code can select classification records based on upper and lower limits specified for any of the classification fields particularly subject_id, workflow_id ,  workflow_version, created_at, gold_standard or expert.  As such it is useful for splitting large classification files into more manageable chunks for example by groups of subjects based on the subject_id  (this is a particularly useful slice since each sub group can be aggregated separately).  This Slicing function is part of the basic frame work for this code rather than a block to be added.  It is necessary to modify the code in the appropriate lines (see the comments in the code) to select the conditional clauses for records to be included in the slice (eg a field to slice on and the upper and lower limits for that field).  Usual the output file name will also be modified to reflect the slice limits.  If no changes are made to the slice conditions of the original code all records in the classification file provided will be flattened and appear in the output file.

The full path and exact file name of the both the classification files to be used, and the output file must be modified at the top of the frame work code.

For each task in the workflow for which the owner wants the volunteer’s input in a simplified format, three things have to be done:
1) The output file field name(s) for the volunteer’s data will chosen and added to the list of the standard output field names already listed.
2) An appropriate block from those provided will be added to the body of the script to extract the data, and a few simple changes made to it so it reads the desired task.  These changes are to change some specified text in the block code to the actual labels or text the volunteers saw when they were given that task.  The same text will be used as the variable name of the data returned by the block.
3) and finally the data returned by each block will be paired with the appropriate output field name in file writer.

Some task blocks are easy – for example a simple question block needs only a snippet of the question asked and returns a single value which is the answer recorded in the classification.  Transcription tasks are similar to simple questions – some part of the transcription task label or instructions will result in the return of the raw input provided by the volunteer for that single task in a single field.  
Drawing tasks can be more complex -  the “answers” may be a list of x,y values (as for points) each with a further sub task answer, or they may even more complex such as circles, ellipses, or rectangles with not only position but size and orientation (angle) values. Polygons each return a list of points for each polygon drawn.  In all cases the project owner will know what data they want and what they want to call each piece so it is a matter of setting up the output file field names and pulling each bit out of the classification download with an appropriate block and pairing it with the output field name.  

Transcriptions  from straight forward text tasks are easy to flatten, but transcription sub- tasks of drawing tools are difficult since it is difficult to know apriori how many columns are needed to break out the data since there are often no limits on the number of drawings made.  In any case it is not trivial to reconcile or aggregate transcriptions.  Notes from Nature has an excellent solution to this problem.  Our drive will be to flatten transcription tasks in a way that NfN reconcile.py can be used directly on the file.

Survey tasks can easily generate many possible question/response options – many choices, many possible how_many bins, and often multiple answer allowed behaviours, and more.  B. Simmons has a solution that flattens and aggregates surveys based on the wotkflow and workflow_contents downloads.  It has some issues and is hard to implement and interpret.  A simpler solution which also can filter the results to resolve and reconcile discrepancies in the volunteer’s data is presented here. 


As well as these standard blocks to pull the volunteers’ data, there are blocks to perform some other basic functions

1)  user_name  This block replaces not-logged-in user_name based on an external picklist 
prepared elsewhere and keyed off user_ip.
For volunteers that did not log in,  user_name is “not-logged-in-” plus the value in user_ip which is some sort of salted hash of the user’s ip address.  Some project owners attempt to identify or group these users by analyzing the user_ip and the platform and browser data in the metadata file.  Elsewhere I provide a simple code that does this and produces a csv file with two fields – user_ip and assigned-user_name.  The function of this block is to accept and read this file into memory and then, if the user_name in the classification file is noted as “not-logged-in…” the block searches for the user_ip in the read file and replaces the user_name with the assigned_user_name from the read file.  The full path and exact filename of the csv file must be modified in the code block if it is to be used.  As well a default user_name for the situation no match is found can be specified otherwise the original “not-logged-in-“plus user_ip is retained.

2) image_number This block attempts to get subject image metadata and generate a image identifier that may be more significant to the project owner.
Many project owners include metadata with their subject manifests that identify the subject images with labels that are more significant to the project – eg actual camera image numbers, location information or file names.  This block is designed to search the subject_data json string of the classification data and pull out specified slices of it based on snippets of text such as ‘filename’, ‘.jpg’, or a particular camera image number format provided by the project owner.
If the metadata was carefully constructed and consistent this can solve the problem of cross referencing the project owner’s image identifiers and the zooniverse subject numbers.  If the metadata is not adequate then an externally built file cross-referencing the subject_id and an image_number can be read into memory and used to add an image_number to the flattened output file. 

3) ellapsed_time  This block pulls the started and finished times from the metadata field and calculates the elapsed time the user spent on the classification.  In many cases this duration is more useful than the individual started and finished times, for example the ellaspsed_time may be very short for bot generated inputs.

4) image_size  This block attempts to pull the natural height and width for the subject image from the metadata file.  This info may be needed later to test for out of bounds drawing tools, correctly scale plots, or scale clustering radii or proximity tests.
This block searches the metadata json and extracts the original image size in pixels. If not present it reverts to a default which can be modified, either to signal the size was not available or to default to a nominal size.

5) test blocks  Various tests of the data extracted from the classification file can be performed.  Unlike the simple blocks discussed above which are effectively independent of each other these test blocks require firstly that the data to be tested has been recovered in a known format and name (ie the appropriate block was set up), and secondly the appropriate test is performed on the appropriate data.  As such they are more difficult to add into the frame work.  At this point only a very limited number of these have been written, specifically:
•	Test points from point drawing tools lay within the image_size ie no out-of-bounds points.
•	Test circle centers are within a fixed percentage of the circle radius of an image edge (ie part of the circle lies within the image.)
•	Test that no two points of the same type are placed within a distance “eps” of each other on the same subject by the same classifier (ie test for double clicks)
•	As for above except for circle centres.
•	Test the radius of a circle is within a range specified (relative to the image_size)
•	Test the ellasped_time is consistent with a human classifier.
